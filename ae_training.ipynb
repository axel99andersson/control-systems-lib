{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Encoder Class\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout, seq_len):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.lstm_enc = nn.LSTM(input_size=input_size, hidden_size=hidden_size, dropout=dropout, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, (last_h_state, last_c_state) = self.lstm_enc(x)\n",
    "        x_enc = last_h_state.squeeze(dim=0)\n",
    "        x_enc = x_enc.unsqueeze(1).repeat(1, x.shape[1], 1)\n",
    "        return x_enc, out\n",
    "\n",
    "\n",
    "# Decoder Class\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout, seq_len, use_act):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "        self.seq_len = seq_len\n",
    "        self.use_act = use_act  # Parameter to control the last sigmoid activation - depends on the normalization used.\n",
    "        self.act = nn.Sigmoid()\n",
    "\n",
    "        self.lstm_dec = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "    def forward(self, z):\n",
    "        # z = z.unsqueeze(1).repeat(1, self.seq_len, 1)\n",
    "        dec_out, (hidden_state, cell_state) = self.lstm_dec(z)\n",
    "        dec_out = self.fc(dec_out)\n",
    "        if self.use_act:\n",
    "            dec_out = self.act(dec_out)\n",
    "\n",
    "        return dec_out, hidden_state\n",
    "\n",
    "\n",
    "# LSTM Auto-Encoder Class\n",
    "class LSTMAE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_ratio, seq_len, use_act=True):\n",
    "        super(LSTMAE, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.encoder = Encoder(input_size=input_size, hidden_size=hidden_size, dropout=dropout_ratio, seq_len=seq_len)\n",
    "        self.decoder = Decoder(input_size=input_size, hidden_size=hidden_size, dropout=dropout_ratio, seq_len=seq_len, use_act=use_act)\n",
    "\n",
    "    def forward(self, x, return_last_h=False, return_enc_out=False):\n",
    "        x_enc, enc_out = self.encoder(x)\n",
    "        x_dec, last_h = self.decoder(x_enc)\n",
    "\n",
    "        if return_last_h:\n",
    "            return x_dec, last_h\n",
    "        elif return_enc_out:\n",
    "            return x_dec, enc_out\n",
    "        return x_dec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, seq_len):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.sequences = []\n",
    "\n",
    "        for _, group in df.groupby(\"experiment\"):\n",
    "            group = group.drop(columns=['time', 'experiment'])\n",
    "            self.sequences.extend(self.create_sequences(group))\n",
    "\n",
    "    def create_sequences(self, group):\n",
    "        sequences = []\n",
    "        for i in range(len(group) - self.seq_len+1):\n",
    "            seq = group.iloc[i:i+self.seq_len].values\n",
    "            sequences.append(torch.tensor(seq, dtype=torch.float32))\n",
    "        \n",
    "        return sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.sequences[index], self.sequences[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/ae_data.csv')\n",
    "df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "df['experiment'] = df.index // 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['pos', 'vel', 'attack', 'attack_pred', 'cusum_stat'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>est_pos</th>\n",
       "      <th>est_vel</th>\n",
       "      <th>det_est_pos</th>\n",
       "      <th>det_est_vel</th>\n",
       "      <th>measured_vel</th>\n",
       "      <th>reference_vel</th>\n",
       "      <th>ctl_signal</th>\n",
       "      <th>residual</th>\n",
       "      <th>experiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.442814</td>\n",
       "      <td>0.159524</td>\n",
       "      <td>16.008867</td>\n",
       "      <td>15.461929</td>\n",
       "      <td>17.442814</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.490442</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.159524</td>\n",
       "      <td>16.008867</td>\n",
       "      <td>0.322726</td>\n",
       "      <td>16.688765</td>\n",
       "      <td>17.360445</td>\n",
       "      <td>17.442814</td>\n",
       "      <td>0.103002</td>\n",
       "      <td>0.704115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.322726</td>\n",
       "      <td>16.688765</td>\n",
       "      <td>0.491437</td>\n",
       "      <td>17.099771</td>\n",
       "      <td>17.521555</td>\n",
       "      <td>17.442814</td>\n",
       "      <td>-0.058896</td>\n",
       "      <td>0.432567</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.491437</td>\n",
       "      <td>17.099771</td>\n",
       "      <td>0.663320</td>\n",
       "      <td>17.303445</td>\n",
       "      <td>17.517581</td>\n",
       "      <td>17.442814</td>\n",
       "      <td>-0.055669</td>\n",
       "      <td>0.214265</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.663320</td>\n",
       "      <td>17.303445</td>\n",
       "      <td>0.834291</td>\n",
       "      <td>16.865072</td>\n",
       "      <td>16.313931</td>\n",
       "      <td>17.442814</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.494729</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   time   est_pos    est_vel  det_est_pos  det_est_vel  measured_vel  \\\n",
       "0  0.00  0.000000  16.442814     0.159524    16.008867     15.461929   \n",
       "1  0.01  0.159524  16.008867     0.322726    16.688765     17.360445   \n",
       "2  0.02  0.322726  16.688765     0.491437    17.099771     17.521555   \n",
       "3  0.03  0.491437  17.099771     0.663320    17.303445     17.517581   \n",
       "4  0.04  0.663320  17.303445     0.834291    16.865072     16.313931   \n",
       "\n",
       "   reference_vel  ctl_signal  residual  experiment  \n",
       "0      17.442814    1.000000 -0.490442           0  \n",
       "1      17.442814    0.103002  0.704115           0  \n",
       "2      17.442814   -0.058896  0.432567           0  \n",
       "3      17.442814   -0.055669  0.214265           0  \n",
       "4      17.442814    1.000000 -0.494729           0  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = SequenceDataset(df[~df.experiment.isin([45, 46, 47, 48, 49])], seq_len=50)\n",
    "val_ds = SequenceDataset(df[df.experiment.isin([45, 46])], seq_len=50)\n",
    "test_ds = SequenceDataset(df[df.experiment.isin([47, 48, 49])], seq_len=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "def train_epoch(model: nn.Module, dataloader: DataLoader, criterion, optimizer, writer: SummaryWriter, epoch: int):\n",
    "    model.train()\n",
    "    for i, (x_batch, y_batch) in enumerate(dataloader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        model_out = model(x_batch)\n",
    "        loss = criterion(model_out, y_batch)\n",
    "        loss.backward()\n",
    "        writer.add_scalar(\"loss/train\", loss.item(), global_step=epoch*len(dataloader)+i)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    return model\n",
    "\n",
    "def eval_model(model: nn.Module, dataloader: DataLoader, criterion, writer: SummaryWriter, epoch: int):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    for i, (x_batch, y_batch) in enumerate(dataloader):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model_out = model(x_batch)\n",
    "            loss = criterion(model_out, y_batch)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    total_loss = total_loss / len(dataloader)\n",
    "    writer.add_scalar(\"loss/validation\", total_loss, global_step=epoch)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(df: pd.DataFrame):\n",
    "    for col in df.columns:\n",
    "        df[col] = (df[col] - df[col].min()) / (df[col].max() - df[col].min())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_156009/3331096375.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = (df[col] - df[col].min()) / (df[col].max() - df[col].min())\n",
      "/tmp/ipykernel_156009/3331096375.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = (df[col] - df[col].min()) / (df[col].max() - df[col].min())\n",
      "/tmp/ipykernel_156009/3331096375.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = (df[col] - df[col].min()) / (df[col].max() - df[col].min())\n",
      "/tmp/ipykernel_156009/3331096375.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = (df[col] - df[col].min()) / (df[col].max() - df[col].min())\n",
      "/tmp/ipykernel_156009/3331096375.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = (df[col] - df[col].min()) / (df[col].max() - df[col].min())\n",
      "/tmp/ipykernel_156009/3331096375.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = (df[col] - df[col].min()) / (df[col].max() - df[col].min())\n",
      "/home/axel/miniconda3/envs/cats/lib/python3.12/site-packages/torch/nn/modules/rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.05 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "Epoch:   0%|          | 0/50 [07:56<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m best_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(NUM_EPOCHS), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 29\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m eval_model(model, val_dataloader, criterion, writer, epoch)\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m val_loss \u001b[38;5;241m<\u001b[39m best_loss:\n",
      "Cell \u001b[0;32mIn[38], line 9\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, criterion, optimizer, writer, epoch)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (x_batch, y_batch) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m      7\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 9\u001b[0m     model_out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(model_out, y_batch)\n\u001b[1;32m     11\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/cats/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cats/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[56], line 58\u001b[0m, in \u001b[0;36mLSTMAE.forward\u001b[0;34m(self, x, return_last_h, return_enc_out)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, return_last_h\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, return_enc_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 58\u001b[0m     x_enc, enc_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     x_dec, last_h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(x_enc)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_last_h:\n",
      "File \u001b[0;32m~/miniconda3/envs/cats/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cats/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[56], line 15\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 15\u001b[0m     out, (last_h_state, last_c_state) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm_enc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     x_enc \u001b[38;5;241m=\u001b[39m last_h_state\u001b[38;5;241m.\u001b[39msqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     17\u001b[0m     x_enc \u001b[38;5;241m=\u001b[39m x_enc\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/cats/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cats/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cats/lib/python3.12/site-packages/torch/nn/modules/rnn.py:891\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    889\u001b[0m unsorted_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 891\u001b[0m     h_zeros \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_directions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mmax_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_hidden_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    894\u001b[0m     c_zeros \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers \u001b[38;5;241m*\u001b[39m num_directions,\n\u001b[1;32m    895\u001b[0m                           max_batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m    896\u001b[0m                           dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    897\u001b[0m     hx \u001b[38;5;241m=\u001b[39m (h_zeros, c_zeros)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv('data/ae_data.csv')\n",
    "df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "df['experiment'] = df.index // 5000\n",
    "df.drop(columns=['pos', 'vel', 'attack', 'attack_pred', 'cusum_stat'], inplace=True)\n",
    "\n",
    "train_ds = SequenceDataset(standardize_data(df[~df.experiment.isin([45, 46, 47, 48, 49])]), seq_len=50)\n",
    "val_ds = SequenceDataset(standardize_data(df[df.experiment.isin([45, 46])]), seq_len=50)\n",
    "test_ds = SequenceDataset(standardize_data(df[df.experiment.isin([47, 48, 49])]), seq_len=50)\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"./tensorboard_logs\")\n",
    "\n",
    "dataloader = DataLoader(train_ds, batch_size=4, shuffle=True)\n",
    "val_dataloader = DataLoader(val_ds, batch_size=4, shuffle=True)\n",
    "# Hyperparams\n",
    "lr = 0.001\n",
    "\n",
    "# Training params\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "# Model, Optimizer, Loss function\n",
    "model = LSTMAE(input_size=8, hidden_size=64, dropout_ratio=0.05, seq_len=5, use_act=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "best_loss = float('inf')\n",
    "for epoch in tqdm(range(NUM_EPOCHS), desc=\"Epoch\"):\n",
    "    model = train_epoch(model, dataloader, criterion, optimizer, writer, epoch)\n",
    "    val_loss = eval_model(model, val_dataloader, criterion, writer, epoch)\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        torch.save(model, f\"./models/ep{epoch}.pt\")\n",
    "        print(f\"Best Validation Loss: {val_loss}\")\n",
    "        best_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3493,  0.2138,  0.4693,  ...,  0.0509, -0.8109, -0.0593],\n",
      "        [ 0.1228, -0.0900,  0.0730,  ..., -0.0366,  0.0244,  0.0203],\n",
      "        [ 0.1838,  0.2163,  0.0255,  ...,  0.3073, -0.4344,  0.7951],\n",
      "        ...,\n",
      "        [ 0.0408, -0.0859,  0.0927,  ..., -0.1854, -2.0009, -0.0689],\n",
      "        [-0.0855, -0.1021,  0.0452,  ..., -0.0476, -0.2275,  0.1095],\n",
      "        [ 0.0551, -0.0714, -0.1079,  ..., -0.1081,  0.1001, -0.0120]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for params in model.parameters():\n",
    "    print(params)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = torch.tensor(df.iloc[:50].values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  0.0000e+00,  1.9000e+01,  1.8620e-01,  1.8676e+01,\n",
       "          1.8240e+01,  1.0000e+00, -3.8015e-01,  0.0000e+00],\n",
       "        [ 1.0000e-02,  1.8620e-01,  1.8676e+01,  3.7516e-01,  1.9276e+01,\n",
       "          1.9863e+01,  1.5582e-01,  6.2173e-01,  0.0000e+00],\n",
       "        [ 2.0000e-02,  3.7516e-01,  1.9276e+01,  5.6845e-01,  1.9449e+01,\n",
       "          1.9574e+01,  4.4961e-01,  1.6605e-01,  0.0000e+00],\n",
       "        [ 3.0000e-02,  5.6845e-01,  1.9449e+01,  7.6258e-01,  1.9388e+01,\n",
       "          1.9240e+01,  7.9096e-01, -8.3876e-02,  0.0000e+00],\n",
       "        [ 4.0000e-02,  7.6258e-01,  1.9388e+01,  9.5752e-01,  1.9693e+01,\n",
       "          1.9998e+01,  3.2545e-02,  3.3730e-01,  0.0000e+00],\n",
       "        [ 5.0000e-02,  9.5752e-01,  1.9693e+01,  1.1562e+00,  2.0180e+01,\n",
       "          2.0757e+01, -7.3405e-01,  5.4815e-01,  0.0000e+00],\n",
       "        [ 6.0000e-02,  1.1562e+00,  2.0180e+01,  1.3562e+00,  1.9654e+01,\n",
       "          1.9016e+01,  1.0000e+00, -5.9683e-01,  0.0000e+00],\n",
       "        [ 7.0000e-02,  1.3562e+00,  1.9654e+01,  1.5534e+00,  1.9857e+01,\n",
       "          2.0068e+01, -3.5554e-02,  2.2781e-01,  0.0000e+00],\n",
       "        [ 8.0000e-02,  1.5534e+00,  1.9857e+01,  1.7496e+00,  1.9200e+01,\n",
       "          1.8432e+01,  1.0000e+00, -7.0403e-01,  0.0000e+00],\n",
       "        [ 9.0000e-02,  1.7496e+00,  1.9200e+01,  1.9422e+00,  1.9401e+01,\n",
       "          1.9548e+01,  5.0514e-01,  2.0571e-01,  0.0000e+00],\n",
       "        [ 1.0000e-01,  1.9422e+00,  1.9401e+01,  2.1347e+00,  1.9019e+01,\n",
       "          1.8524e+01,  1.0000e+00, -4.0911e-01,  0.0000e+00],\n",
       "        [ 1.1000e-01,  2.1347e+00,  1.9019e+01,  2.3242e+00,  1.8879e+01,\n",
       "          1.8627e+01,  1.0000e+00, -1.5284e-01,  0.0000e+00],\n",
       "        [ 1.2000e-01,  2.3242e+00,  1.8879e+01,  2.5129e+00,  1.8898e+01,\n",
       "          1.8805e+01,  1.0000e+00,  1.2724e-02,  0.0000e+00],\n",
       "        [ 1.3000e-01,  2.5129e+00,  1.8898e+01,  2.7035e+00,  1.9371e+01,\n",
       "          1.9816e+01,  2.7880e-01,  5.1181e-01,  0.0000e+00],\n",
       "        [ 1.4000e-01,  2.7035e+00,  1.9371e+01,  2.8979e+00,  1.9578e+01,\n",
       "          1.9748e+01,  3.4923e-01,  2.2191e-01,  0.0000e+00],\n",
       "        [ 1.5000e-01,  2.8979e+00,  1.9578e+01,  3.0952e+00,  2.0015e+01,\n",
       "          2.0505e+01, -4.1256e-01,  4.8932e-01,  0.0000e+00],\n",
       "        [ 1.6000e-01,  3.0952e+00,  2.0015e+01,  3.2950e+00,  1.9924e+01,\n",
       "          1.9803e+01,  2.9162e-01, -1.0650e-01,  0.0000e+00],\n",
       "        [ 1.7000e-01,  3.2950e+00,  1.9924e+01,  3.4942e+00,  1.9913e+01,\n",
       "          1.9881e+01,  2.1433e-01, -1.4000e-02,  0.0000e+00],\n",
       "        [ 1.8000e-01,  3.4942e+00,  1.9913e+01,  3.6949e+00,  2.0341e+01,\n",
       "          2.0866e+01, -7.7913e-01,  4.8542e-01,  0.0000e+00],\n",
       "        [ 1.9000e-01,  3.6949e+00,  2.0341e+01,  3.8944e+00,  1.9143e+01,\n",
       "          1.7832e+01,  1.0000e+00, -1.2743e+00,  0.0000e+00],\n",
       "        [ 2.0000e-01,  3.8944e+00,  1.9143e+01,  4.0841e+00,  1.8718e+01,\n",
       "          1.8182e+01,  1.0000e+00, -4.6230e-01,  0.0000e+00],\n",
       "        [ 2.1000e-01,  4.0841e+00,  1.8718e+01,  4.2716e+00,  1.8858e+01,\n",
       "          1.8886e+01,  1.0000e+00,  1.2085e-01,  0.0000e+00],\n",
       "        [ 2.2000e-01,  4.2716e+00,  1.8858e+01,  4.4619e+00,  1.9354e+01,\n",
       "          1.9816e+01,  3.2364e-01,  5.2571e-01,  0.0000e+00],\n",
       "        [ 2.3000e-01,  4.4619e+00,  1.9354e+01,  4.6551e+00,  1.9320e+01,\n",
       "          1.9178e+01,  9.7005e-01, -5.6298e-02,  0.0000e+00],\n",
       "        [ 2.4000e-01,  4.6551e+00,  1.9320e+01,  4.8503e+00,  1.9847e+01,\n",
       "          2.0408e+01, -2.6452e-01,  5.8713e-01,  0.0000e+00],\n",
       "        [ 2.5000e-01,  4.8503e+00,  1.9847e+01,  5.0476e+00,  1.9567e+01,\n",
       "          1.9179e+01,  9.7307e-01, -3.2120e-01,  0.0000e+00],\n",
       "        [ 2.6000e-01,  5.0476e+00,  1.9567e+01,  5.2455e+00,  2.0167e+01,\n",
       "          2.0854e+01, -7.1071e-01,  6.7708e-01,  0.0000e+00],\n",
       "        [ 2.7000e-01,  5.2455e+00,  2.0167e+01,  5.4489e+00,  2.0641e+01,\n",
       "          2.1236e+01, -1.0000e+00,  5.2955e-01,  0.0000e+00],\n",
       "        [ 2.8000e-01,  5.4489e+00,  2.0641e+01,  5.6531e+00,  1.9993e+01,\n",
       "          1.9247e+01,  8.9202e-01, -7.3016e-01,  0.0000e+00],\n",
       "        [ 2.9000e-01,  5.6531e+00,  1.9993e+01,  5.8521e+00,  1.9733e+01,\n",
       "          1.9389e+01,  7.5582e-01, -2.9386e-01,  0.0000e+00],\n",
       "        [ 3.0000e-01,  5.8521e+00,  1.9733e+01,  6.0490e+00,  1.9640e+01,\n",
       "          1.9473e+01,  6.7698e-01, -1.0484e-01,  0.0000e+00],\n",
       "        [ 3.1000e-01,  6.0490e+00,  1.9640e+01,  6.2445e+00,  1.9426e+01,\n",
       "          1.9100e+01,  1.0000e+00, -2.3873e-01,  0.0000e+00],\n",
       "        [ 3.2000e-01,  6.2445e+00,  1.9426e+01,  6.4391e+00,  1.9558e+01,\n",
       "          1.9631e+01,  5.3144e-01,  1.4609e-01,  0.0000e+00],\n",
       "        [ 3.3000e-01,  6.4391e+00,  1.9558e+01,  6.6362e+00,  1.9984e+01,\n",
       "          2.0449e+01, -2.9064e-01,  4.8184e-01,  0.0000e+00],\n",
       "        [ 3.4000e-01,  6.6362e+00,  1.9984e+01,  6.8346e+00,  1.9611e+01,\n",
       "          1.9126e+01,  1.0000e+00, -4.2045e-01,  0.0000e+00],\n",
       "        [ 3.5000e-01,  6.8346e+00,  1.9611e+01,  7.0354e+00,  2.0922e+01,\n",
       "          2.2355e+01, -1.0000e+00,  1.4043e+00,  0.0000e+00],\n",
       "        [ 3.6000e-01,  7.0354e+00,  2.0922e+01,  7.2429e+00,  2.0361e+01,\n",
       "          1.9760e+01,  3.8616e-01, -5.9576e-01,  0.0000e+00],\n",
       "        [ 3.7000e-01,  7.2429e+00,  2.0361e+01,  7.4486e+00,  2.0971e+01,\n",
       "          2.1702e+01, -1.0000e+00,  6.7314e-01,  0.0000e+00],\n",
       "        [ 3.8000e-01,  7.4486e+00,  2.0971e+01,  7.6575e+00,  2.0672e+01,\n",
       "          2.0411e+01, -2.8673e-01, -3.0856e-01,  0.0000e+00],\n",
       "        [ 3.9000e-01,  7.6575e+00,  2.0672e+01,  7.8615e+00,  1.9816e+01,\n",
       "          1.8849e+01,  1.0000e+00, -9.3558e-01,  0.0000e+00],\n",
       "        [ 4.0000e-01,  7.8615e+00,  1.9816e+01,  8.0578e+00,  1.9318e+01,\n",
       "          1.8708e+01,  1.0000e+00, -5.3811e-01,  0.0000e+00],\n",
       "        [ 4.1000e-01,  8.0578e+00,  1.9318e+01,  8.2522e+00,  1.9680e+01,\n",
       "          2.0032e+01,  1.1667e-01,  3.9307e-01,  0.0000e+00],\n",
       "        [ 4.2000e-01,  8.2522e+00,  1.9680e+01,  8.4485e+00,  1.9565e+01,\n",
       "          1.9362e+01,  7.9328e-01, -1.3863e-01,  0.0000e+00],\n",
       "        [ 4.3000e-01,  8.4485e+00,  1.9565e+01,  8.6452e+00,  1.9881e+01,\n",
       "          2.0209e+01, -5.5838e-02,  3.5423e-01,  0.0000e+00],\n",
       "        [ 4.4000e-01,  8.6452e+00,  1.9881e+01,  8.8458e+00,  2.0361e+01,\n",
       "          2.0938e+01, -7.9461e-01,  5.4180e-01,  0.0000e+00],\n",
       "        [ 4.5000e-01,  8.8458e+00,  2.0361e+01,  9.0500e+00,  2.0518e+01,\n",
       "          2.0751e+01, -6.1464e-01,  1.7710e-01,  0.0000e+00],\n",
       "        [ 4.6000e-01,  9.0500e+00,  2.0518e+01,  9.2538e+00,  2.0094e+01,\n",
       "          1.9614e+01,  5.2606e-01, -4.7991e-01,  0.0000e+00],\n",
       "        [ 4.7000e-01,  9.2538e+00,  2.0094e+01,  9.4570e+00,  2.0751e+01,\n",
       "          2.1530e+01, -1.0000e+00,  7.1804e-01,  0.0000e+00],\n",
       "        [ 4.8000e-01,  9.4570e+00,  2.0751e+01,  9.6633e+00,  2.0348e+01,\n",
       "          1.9927e+01,  1.9890e-01, -4.4269e-01,  0.0000e+00],\n",
       "        [ 4.9000e-01,  9.6633e+00,  2.0348e+01,  9.8656e+00,  2.0000e+01,\n",
       "          1.9594e+01,  5.3540e-01, -3.8753e-01,  0.0000e+00]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 7, got 9",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdp\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cats/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cats/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 58\u001b[0m, in \u001b[0;36mLSTMAE.forward\u001b[0;34m(self, x, return_last_h, return_enc_out)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, return_last_h\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, return_enc_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 58\u001b[0m     x_enc, enc_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     x_dec, last_h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(x_enc)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_last_h:\n",
      "File \u001b[0;32m~/miniconda3/envs/cats/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cats/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 15\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 15\u001b[0m     out, (last_h_state, last_c_state) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm_enc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     x_enc \u001b[38;5;241m=\u001b[39m last_h_state\u001b[38;5;241m.\u001b[39msqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     17\u001b[0m     x_enc \u001b[38;5;241m=\u001b[39m x_enc\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/cats/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cats/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cats/lib/python3.12/site-packages/torch/nn/modules/rnn.py:898\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    894\u001b[0m     c_zeros \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers \u001b[38;5;241m*\u001b[39m num_directions,\n\u001b[1;32m    895\u001b[0m                           max_batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m    896\u001b[0m                           dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    897\u001b[0m     hx \u001b[38;5;241m=\u001b[39m (h_zeros, c_zeros)\n\u001b[0;32m--> 898\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_batched:\n",
      "File \u001b[0;32m~/miniconda3/envs/cats/lib/python3.12/site-packages/torch/nn/modules/rnn.py:827\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_forward_args\u001b[39m(\u001b[38;5;28mself\u001b[39m,  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m    823\u001b[0m                        \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[1;32m    824\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[1;32m    825\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[1;32m    826\u001b[0m                        ):\n\u001b[0;32m--> 827\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[1;32m    829\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[0] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    830\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[1;32m    831\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/cats/lib/python3.12/site-packages/torch/nn/modules/rnn.py:246\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput must have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_input_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 246\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 7, got 9"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "model(dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
